read_csv("4_1_data.csv")
datos<- read_csv("4_1_data.csv")
datos <- read_csv("4_1_data.csv")
datos <- read_csv("4_1_data.csv", sep=";")
datos <- read_csv("4_1_data.csv", sep=";", header=T)
datos <- read.csv2("4_1_data.csv", sep=";", header=T)
View(datos)
View(datos)
library(MASS)
datA <- read.csv2("4_1_data.csv", sep=";", header=T)
data<- read.csv2("4_1_data.csv", sep=";", header=T)
data
data<- read.csv2("4_1_data.csv", sep=";", header=T, dec=".")
data
data<- read.csv2("4_1_data.csv", sep=";", header=T, dec=".")
data
View(datA)
View(datA)
data<- read.csv2("4_1_data.csv", sep=";", header=T, dec=".")
View(data)
View(data)
data<- read.csv("4_1_data.csv", sep=",", header=T, dec=".")
data
data<- read.csv("4_1_data.csv", sep=",", header=T, dec=".")
data
data
summary(data)
##Ahora hay que generar la parte de train y la de testeo
nrow(data)
##Creamos la parte de train y la de testeo
ntrain <- nrow(data)*0.7
ntest <- nrow(data)*0.3
##Obsevamos el numero de testeo y de entrenamiento
c(ntrain,ntest)
##Establecemos una semilla
set.seed(123)
index_train <- sample(1:nrow(data), size =ntrain)
train <- data[index_train,]
test <- data[-index_train,]
summary(train)
logit_reg <- glm(label~.,data = train,family = "binomial")
lda_fit<-lda(label~.,data=train)
predicted_value <- predict(logit_reg,test,type = "response")
predicted_class <- ifelse(predicted_value>0.5, "Yes","No")
performance_data<-data.frame(observed=test$label,
predicted= predicted_class)
positive <- sum(performance_data$observed=="Yes")
negative <- sum(performance_data$observed=="No")
predicted_positive <- sum(performance_data$predicted=="Yes")
predicted_negative <- sum(performance_data$predicted=="No")
total <- nrow(performance_data)
data.frame(positive, negative,predicted_positive,predicted_negative)
data
summary(data)
##Ahora hay que generar la parte de train y la de testeo
nrow(data)
##Creamos la parte de train y la de testeo
ntrain <- nrow(data)*0.7
ntest <- nrow(data)*0.3
##Obsevamos el numero de testeo y de entrenamiento
c(ntrain,ntest)
##Establecemos una semilla
set.seed(123)
index_train <- sample(1:nrow(data), size =ntrain)
train <- data[index_train,]
test <- data[-index_train,]
summary(train)
logit_reg <- glm(label~.,data = train,family = "binomial")
lda_fit<-lda(label~.,data=train)
predicted_value <- predict(logit_reg,test,type = "response")
predicted_class <- ifelse(predicted_value>0.5, "Yes","No")
performance_data<-data.frame(observed=test$label,
predicted= predicted_class)
positive <- sum(performance_data$observed=="Yes")
negative <- sum(performance_data$observed=="No")
predicted_positive <- sum(performance_data$predicted=="Yes")
predicted_negative <- sum(performance_data$predicted=="No")
total <- nrow(performance_data)
data.frame(positive, negative,predicted_positive,predicted_negative)
datos<- read.csv("4_1_data.csv")
View(datos)
View(datos)
data<- read.csv("4_1_data.csv")
plot(data$score.1, data$score.2, col = as.factor(data$label), xlab = "Score-1", ylab = "Score-2")
data<- read.csv("4_1_data.csv")
str(data)
plot(data$score.1, data$score.2, col = as.factor(data$label), xlab = "Score-1", ylab = "Score-2")
str(data)
#Predictor variables
X <- as.matrix(data[, c(1,2)])
X <- cbind(rep(1, nrow(X)), X)
View(data)
View(data)
Y <- as.matrix(data$label)
initial_parameters <- rep(0, ncol(X))
CostFunction(initial_parameters, X, Y)
library(testthat)
library(gradDescent)
CostFunction(initial_parameters, X, Y)
set.seed(123)
trainning_data<-data.class(label)
TestGradientDescent2 <- function(iterations = 1200, learning_rate = 0.25, the_data) {
# label in the last column in dataSet
model <- gradDescentR.learn(dataSet = data, featureScaling = TRUE, scalingMethod = "VARIANCE",
learningMethod = "GD", control = list(alpha = learning_rate, maxIter = iterations),
seed = 1234)
model
}
TestGradientDescent2()
TestGradientDescent2(daa)
TestGradientDescent2(data)
TestGradientDescent2(data)
initial_parameters <- rep(0, ncol(X))
set.seed(123)
TestGradientDescent2 <- function(iterations = 1200, learning_rate = 0.25, the_data) {
# label in the last column in dataSet
model <- gradDescentR.learn(dataSet = data, featureScaling = TRUE, scalingMethod = "VARIANCE",
learningMethod = "GD", control = list(alpha = learning_rate, maxIter = iterations),
seed = 1234)
model
}
TestGradientDescent2(data)
TestGradientDescent2()
test_that("Test TestGradientDescent",{
parameters <- TestGradientDescent(X = X, Y = Y)
# probability of admission for student (1 = b, for the calculos)
new_student <- c(1,25,78)
prob_new_student <- Sigmoid(t(new_student) %*% parameters)
print(prob_new_student)
expect_equal(as.numeric(round(prob_new_student, digits = 4)), 0.0135)
# Fail, test
# expect_equal(as.numeric(round(prob_new_student, digits = 4)), 0.0130)
})
("4_1_data.csv")
data<- read.csv("4_1_data.csv")
plot(data$score.1, data$score.2, col = as.factor(data$label), xlab = "Score-1", ylab = "Score-2")
data_clean <- subset(data, select = c(score.1, score.2))
data_clean <- cbind(initial = 1, data_clean)
dfProb <- 0
for(i in 1:nrow(data_clean)){
res <- Sigmoid(t(as.numeric(data_clean[i,])) %*% parameters)
dfProb <- rbind(dfProb, res)
}
library(sigmoid)
install.packages("sigmoid")
library(sigmoid)
dfProb <- 0
for(i in 1:nrow(data_clean)){
res <- Sigmoid(t(as.numeric(data_clean[i,])) %*% parameters)
dfProb <- rbind(dfProb, res)
}
library(sigmoid)
for(i in 1:nrow(data_clean)){
res <- Sigmoid(t(as.numeric(data_clean[i,])) %*% parameters)
dfProb <- rbind(dfProb, res)
}
dfProb
#La primera linea de este dataframe no tiene info
dfProb <- as.data.frame(dfProb)
dfProb <- dfProb[-1,]
#Dataframe original + las probabilidades
data_complete <- cbind(data, prob = dfProb)
View(data_complete)
table(data_complete$label, ifelse(data_complete$prob > 0.68, 1, 0))
data_clean <- cbind(initial = 1, data_clean)
Sigmoid <- function(x) {
1 / (1 + exp(-x))
}
dfProb <- 0
for(i in 1:nrow(data_clean)){
res <- Sigmoid(t(as.numeric(data_clean[i,])) %*% parameters)
dfProb <- rbind(dfProb, res)
}
data_clean <- cbind(initial = 1, data_clean)
Sigmoid <- function(x) {
1 / (1 + exp(-x))
}
parameters <- rep(0, ncol(X))
data_clean <- subset(data, select = c(score.1, score.2))
data_clean <- cbind(initial = 1, data_clean)
X <- as.matrix(data[, c(1,2)])
X <- cbind(rep(1, nrow(X)), X)
Y <- as.matrix(data$label)
initial_parameters <- rep(0, ncol(X))
CostFunction(initial_parameters, X, Y)
Sigmoid <- function(x) {
1 / (1 + exp(-x))
}
parameters <- rep(0, ncol(X))
knitr::opts_chunk$set(echo = TRUE)
Sigmoid <- function(x) {
1 / (1 + exp(-x))
}
# feed with data
x <- seq(-5, 5, 0.01)
# and plot
plot(x, Sigmoid(x), col='blue', ylim = c(-.2, 1))
abline(h = 0, v = 0, col = "gray60")
# Ref: https://www.r-bloggers.com/logistic-regression-with-r-step-by-step-implementation-part-2/
# Cost Function
#
CostFunction <- function(parameters, X, Y) {
n <- nrow(X)
# function to apply (%*% Matrix multiplication)
g <- Sigmoid(X %*% parameters)
J <- (1/n) * sum((-Y * log(g)) - ((1 - Y) * log(1 - g)))
return(J)
}
#Load data
data <- read.csv("data/4_1_data.csv")
#Create plot
plot(data$score.1, data$score.2, col = as.factor(data$label), xlab = "Score-1", ylab = "Score-2")
#Predictor variables
X <- as.matrix(data[, c(1,2)])
#Add ones to X in the first column (matrix multiplication x b)
X <- cbind(rep(1, nrow(X)), X)
#Response variable
Y <- as.matrix(data$label)
#Intial parameters
initial_parameters <- rep(0, ncol(X))
#Cost at inital parameters
CostFunction(initial_parameters, X, Y)
#Intial parameters
initial_parameters <- rep(0, ncol(X))
#Cost at inital parameters
CostFunction(initial_parameters, X, Y)
# We want to minimize the cost function. Then derivate this funcion
TestGradientDescent <- function(iterations = 1000, X, Y) {
# Initialize (b, W)
parameters <- rep(0, ncol(X))
# Check evolution
print(paste("Initial Cost Function value: ",
convergence <- c(CostFunction(parameters, X, Y)), sep = ""))
# updating (b, W) using gradient update
# Derive theta using gradient descent using optim function
# Look for information about the "optim" function (there are other options)
parameters_optimization <- optim(par = parameters, fn = CostFunction, X = X, Y = Y,
control = list(maxit = iterations))
#set parameters
parameters <- parameters_optimization$par
# Check evolution
print(paste("Final Cost Function value: ",
convergence <- c(CostFunction(parameters, X, Y)), sep = ""))
return(parameters)
}
# How to use
parameters <- TestGradientDescent(X = X, Y = Y)
# probability of admission for student (1 = b, for the calculos)
new_student <- c(1,25,78)
print("Probability of admission for student:")
print(prob_new_student <- Sigmoid(t(new_student) %*% parameters))
if (!require("gradDescent")) install.packages("gradDescent")
# load
library("gradDescent")
# We want to minimize the cost function. Then derivate this funcion
TestGradientDescent2 <- function(iterations = 1200, learning_rate = 0.25, the_data) {
# label in the last column in dataSet
model <- gradDescentR.learn(dataSet = the_data, featureScaling = TRUE, scalingMethod = "VARIANCE",
learningMethod = "GD", control = list(alpha = learning_rate, maxIter = iterations),
seed = 1234)
model
}
# How to use
TestGradientDescent2(the_data = data)
# Now, the exercises. Use training and test set, change the value of alpha...
# Install if not installed
if (!require("testthat")) install.packages("testthat")
# load
library(testthat)
test_that("Test TestGradientDescent",{
parameters <- TestGradientDescent(X = X, Y = Y)
# probability of admission for student (1 = b, for the calculos)
new_student <- c(1,25,78)
prob_new_student <- Sigmoid(t(new_student) %*% parameters)
print(prob_new_student)
expect_equal(as.numeric(round(prob_new_student, digits = 4)), 0.0135)
# Fail, test
# expect_equal(as.numeric(round(prob_new_student, digits = 4)), 0.0130)
})
data<- read.csv("4_1_data.csv")
data<- read.csv("C:/Users/Usuario/Desktop/MASTER IN DATA SCIENCE/MACHINE LEARNING/DATA4_1_data.csv")
data<- read.csv("C:/Users/Usuario/Desktop/MASTER IN DATA SCIENCE/MACHINE LEARNING/4_1_data.csv")
data<- read.csv("C:/Users/Usuario/Desktop/MASTER IN DATA SCIENCE/MACHINE LEARNING/DATA/4_1_data.csv")
data_clean <- subset(data, select = c(score.1, score.2))
data_clean <- cbind(initial = 1, data_clean)
dfProb <- 0
for(i in 1:nrow(data_clean)){
res <- Sigmoid(t(as.numeric(data_clean[i,])) %*% parameters)
dfProb <- rbind(dfProb, res)
}
dfProb
#La primera linea de este dataframe no tiene info
dfProb <- as.data.frame(dfProb)
dfProb <- dfProb[-1,]
#Dataframe original + las probabilidades
data_complete <- cbind(data, prob = dfProb)
View(data_complete)
table(data_complete$label, ifelse(data_complete$prob > 0.68, 1, 0))
data<- read.csv("C:/Users/Usuario/Desktop/MASTER IN DATA SCIENCE/MACHINE LEARNING/DATA/4_1_data.csv")
data_clean <- subset(data, select = c(score.1, score.2))
data_clean <- cbind(initial = 1, data_clean)
dfProb <- 0
for(i in 1:nrow(data_clean)){
res <- Sigmoid(t(as.numeric(data_clean[i,])) %*% parameters)
dfProb <- rbind(dfProb, res)
}
dfProb
#La primera linea de este dataframe no tiene info
dfProb <- as.data.frame(dfProb)
dfProb <- dfProb[-1,]
#Dataframe original + las probabilidades
data_complete <- cbind(data, prob = dfProb)
table(data_complete$label, ifelse(data_complete$prob > 0.68, 1, 0))
#Load data
data <- read.csv("data/4_1_data.csv")
#Load data
data <- read.csv("4_1_data.csv")
Sigmoid <- function(x) {
1 / (1 + exp(-x))
}
CostFunction <- function(parameters, X, Y) {
n <- nrow(X)
# function to apply (%*% Matrix multiplication)
g <- Sigmoid(X %*% parameters)
J <- (1/n) * sum((-Y * log(g)) - ((1 - Y) * log(1 - g)))
return(J)
}
TestGradientDescent <- function(iterations = 10000, X, Y) {
# Initialize (b, W)
parameters <- rep(0, ncol(X))
# Check evolution
print(paste("Initial Cost Function value: ",
convergence <- c(CostFunction(parameters, X, Y)), sep = ""))
# updating (b, W) using gradient update
# Derive theta using gradient descent using optim function
# Look for information about the "optim" function (there are other options)
parameters_optimization <- optim(par = parameters, fn = CostFunction, X = X, Y = Y,
control = list(maxit = iterations))
#set parameters
parameters <- parameters_optimization$par
# Check evolution
print(paste("Final Cost Function value: ",
convergence <- c(CostFunction(parameters, X, Y)), sep = ""))
return(parameters)
}
X <- as.matrix(data[, c(1,2)])
X <- cbind(rep(1, nrow(X)), X)
Y <- as.matrix(data$label)
parameters <- TestGradientDescent(X = X, Y = Y)
data_clean <- subset(data, select = c(score.1, score.2))
data_clean <- cbind(initial = 1, data_clean)
dfProb <- 0
for(i in 1:nrow(data_clean)){
res <- Sigmoid(t(as.numeric(data_clean[i,])) %*% parameters)
dfProb <- rbind(dfProb, res)
}
#La primera linea de este dataframe no tiene info
dfProb <- as.data.frame(dfProb)
dfProb <- dfProb[-1,]
#Dataframe original + las probabilidades
data_complete <- cbind(data, prob = dfProb)
table(data_complete$label, ifelse(data_complete$prob > 0.68, 1, 0))
it_error <- cbind(0,0)
colnames(it_error) <- c("Iteraciones","Convergencia")
set.seed(123)
for (i in seq(1:750)) {
parameters <- TestGradientDescent(iterations = i, X = X, Y = Y)
convergence <- c(CostFunction(parameters, X, Y))
output <- cbind(i,round(convergence,5))
colnames(output) <- c("iterations","convergence")
it_error <- rbind(it_error,output)
}
library(ggplot2)
it_error <- as.data.frame(it_error)
ggplot(it_error,aes(x=it_error$Iteraciones ,y=it_error$Convergencia)) + geom_smooth() + xlab('Iteraciones') + ylab('Convergencia')
